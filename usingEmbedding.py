import os
import time
import streamlit as st
import google.generativeai as genai
import numpy as np
import faiss
from dotenv import load_dotenv

# --- CONFIGURA√á√ÉO INICIAL ---
st.set_page_config(
    page_title="ZenithBot - O seu assistente",
    page_icon="ü§ñ",
    layout="wide"
)

load_dotenv()

# --- CONSTANTES E MODELOS ---

# Prompt do sistema para guiar o modelo de linguagem
PROMPT_TEMPLATE = """
Voc√™ √© o ZenithBot, um assistente especializado no software de gest√£o Zenith.
Sua fun√ß√£o √© responder perguntas baseando-se estritamente no contexto fornecido.
Seja claro, objetivo e use apenas as informa√ß√µes dispon√≠veis.

Se a resposta n√£o estiver no contexto, responda:
"Desculpe, n√£o encontrei essa informa√ß√£o nos meus documentos."

CONTEXTO:
{contexto}

PERGUNTA:
{pergunta}
"""

EMBEDDING_MODEL = 'text-embedding-004'

# --- FUN√á√ïES PRINCIPAIS ---

def configure_genai():
    """Configura a API do Google Generative AI com a chave fornecida."""
    api_key = os.getenv("GENAI_API_KEY")
    if not api_key:
        st.error("A vari√°vel de ambiente GENAI_API_KEY n√£o foi definida. Por favor, configure-a no seu arquivo .env.")
        st.stop()
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-1.5-flash')

def split_text_into_chunks(text: str) -> list[str]:
    """Divide um texto longo em peda√ßos menores (chunks) usando a quebra de linha."""
    parts = text.split("\n")
    # Remove espa√ßos em branco e garante que chunks vazios n√£o sejam inclu√≠dos
    chunks = [part.strip() for part in parts if part.strip()]
    return chunks

@st.cache_resource(show_spinner="Analisando o documento e preparando a base de conhecimento...")
def create_faiss_index():
    """
    L√™ o documento de contexto, gera embeddings para os chunks de texto
    e cria um √≠ndice FAISS para busca r√°pida de similaridade.
    """
    context_path = os.getenv('CONTEXT_PATH')

    if not context_path or not os.path.exists(context_path):
        st.error(
            f"Arquivo de contexto n√£o encontrado. "
            f"Verifique se a vari√°vel 'CONTEXT_PATH' no seu arquivo .env aponta para um arquivo v√°lido."
        )
        st.stop()

    with open(context_path, 'r', encoding='utf-8') as f:
        document_text = f.read()

    text_chunks = split_text_into_chunks(document_text)

    # Gera embeddings para os chunks de texto
    response = genai.embed_content(model=EMBEDDING_MODEL, content=text_chunks)
    embeddings = np.array(response['embedding'], dtype='float32')

    # Normaliza os vetores para usar similaridade de cosseno (com produto interno)
    faiss.normalize_L2(embeddings)

    # Cria o √≠ndice FAISS
    embedding_dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(embedding_dimension)
    index.add(embeddings)

    return index, text_chunks

def retrieve_relevant_contexts(query: str, index: faiss.Index, text_chunks: list, top_k: int = 3) -> list[str]:
    """Busca os chunks de texto mais relevantes para a pergunta do usu√°rio no √≠ndice FAISS."""
    query_embedding_response = genai.embed_content(model=EMBEDDING_MODEL, content=query)
    query_vector = np.array(query_embedding_response['embedding'], dtype='float32').reshape(1, -1)
    faiss.normalize_L2(query_vector)

    _, indices = index.search(query_vector, top_k)
    
    # Retorna os textos correspondentes, ignorando resultados inv√°lidos (-1)
    return [text_chunks[i] for i in indices[0] if i != -1]

def generate_response_with_llm(prompt: str, llm_model) -> str:
    try:
        response = llm_model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"Ocorreu um erro ao gerar a resposta: {e}")
        return "Desculpe, tive um problema ao processar sua solicita√ß√£o."

def simulate_typing_effect(text: str):
    """Exibe o texto com um efeito de digita√ß√£o e um cursor."""
    message_placeholder = st.empty()
    full_response = ""
    for chunk in text.split():
        full_response += chunk + " "
        time.sleep(0.05)
        message_placeholder.markdown(full_response + "‚ñå")
    message_placeholder.markdown(full_response.strip())


# --- INTERFACE DO STREAMLIT ---

st.title("ü§ñ ZenithBot - Assistente Virtual")
st.caption("Fa√ßa uma pergunta sobre o software de gest√£o Zenith e eu responderei com base no meu conhecimento.")

# Configura o modelo e cria o √≠ndice (executado apenas uma vez)
llm_model = configure_genai()
faiss_index, base_texts = create_faiss_index()

# Inicializa o hist√≥rico de chat no estado da sess√£o
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Como posso ajudar voc√™ a entender o Zenith hoje?"}]

# Exibe o hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a entrada do usu√°rio
if user_input := st.chat_input("Qual √© a sua d√∫vida?"):
    # Adiciona e exibe a mensagem do usu√°rio
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # Processa a resposta do assistente
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            # 1. Recupera contextos relevantes
            relevant_contexts = retrieve_relevant_contexts(user_input, faiss_index, base_texts)
            
            # 2. Constr√≥i o prompt para o LLM
            if relevant_contexts:
                context_str = "\n\n---\n\n".join(relevant_contexts)
                final_prompt = PROMPT_TEMPLATE.format(contexto=context_str, pergunta=user_input)
            else:
                final_prompt = user_input

            # 3. Gera a resposta
            response_text = generate_response_with_llm(final_prompt, llm_model)

            # 4. Exibe a resposta com efeito de digita√ß√£o
            simulate_typing_effect(response_text)
            
    # 5. Adiciona a resposta do assistente ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response_text})

